{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1739fab",
   "metadata": {},
   "source": [
    "# Study Note for  Feature Engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd3302d",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Mutual Information ([**link**](https://www.kaggle.com/ryanholbrook/mutual-information))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b27d57",
   "metadata": {},
   "source": [
    "### 1) ‚ùì What is Mutual Information?\n",
    "#### (1) ü§Ø Intuition\n",
    "   In previous article, we said that apparent temperature produce **information** about coat sales. Let's think fundamentally. Why did we say 'apparent temperature produce **information**'? What is the **information**?<br>\n",
    "     \n",
    "   If we observe apparent temperature, coat sales will become less random than before, though we still don't know exactly how much coats will be sold. For example, let suppose that observed apparent temperature of this month is much lower than last month. Then we intuitively expect that coat sales of this month may be higher than last month. This is because the possibility that coat sales of this month become lower than last month decreases and the opposite possibility become higher. It is like pruning. Cutting possible cases, we could predict better what will happen in the future. And in this context we say that apparent temperature produce information. **The core of information is decrease in randomness. This is called dependency in statistics.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d59b356",
   "metadata": {},
   "source": [
    "#### (2) üìè Entropy : Measurement of randomness\n",
    "   The randomness of some random variable X is often measured by the concept **Entropy**. **Entropy is the number of bits on average needed to store a draw from X.**   \n",
    "     \n",
    "   Yeah, this definition is difficult to understandüòÇ. When I first saw this definition, it was hard for me to understand why bit is mentioned all of a sudden. To help to understand, let imagine two simple random experiments. The first experiment is flipping a coin, and the second is flipping two coins. Which experiment is more **random**? Of course, it's the latter. It is because there are more 'branches' in the second experiment(=4) than the first experiment(=2). The point where branches are divided is a bit. The more random a random variable X is, the more bits exist in 'tree' of X, and, therefore, the more bits is needed to store a draw from X. It is the reason of using a bit as unit to measure the randomness of X.  \n",
    "     \n",
    "   The mathematical definition of Entropy is as follow:  \n",
    "  \n",
    "   ![entropy](https://www.researchgate.net/profile/Jodi-Kearns/publication/34995295/figure/fig16/AS:669479310729234@1536627714604/Shannons-original-Entropy-equation.png)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a75f015",
   "metadata": {},
   "source": [
    "#### (3) üîó Mutual Information : Measurement of Dependency\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
